import json
import os
from pathlib import Path
from typing import List, Dict, Any

from dotenv import load_dotenv
import yaml
from supabase import create_client
from tqdm import tqdm

import torch
from sentence_transformers import CrossEncoder

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda


# ==================================================
# 0. í™˜ê²½ ë¡œë“œ + LangSmith ì„¤ì •
# ==================================================
BASE_DIR = Path(__file__).resolve().parents[2]
load_dotenv(BASE_DIR / ".env")

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "rfp-rag-eval")


# ==================================================
# 1. Supabase / Embedding / LLM / Reranker
# ==================================================
supabase = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY"),
)

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

llm = ChatOpenAI(
    model=os.getenv("OPENAI_LLM_MODEL", "gpt-4o-mini"),
    temperature=0,
)

RERANKER_MODEL = os.getenv(
    "RERANKER_MODEL",
    "dragonkue/bge-reranker-v2-m3-ko",
)
device = "cuda" if torch.cuda.is_available() else "cpu"
reranker = CrossEncoder(RERANKER_MODEL, device=device)


# ==================================================
# 2. Prompt YAML ë¡œë”©
# ==================================================
PROMPT_PATH = BASE_DIR / "src" / "prompts" / "ragas_template.yaml"
with open(PROMPT_PATH, "r", encoding="utf-8") as f:
    prompt_yaml = yaml.safe_load(f)["prompt"]

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", prompt_yaml["system"]),
        (
            "human",
            prompt_yaml["instructions"]
            + "\n\n"
            + prompt_yaml["context_format"]
            + "\n\n"
            + prompt_yaml["user_prompt"]
            + "\n\n"
            + prompt_yaml["answer_guidelines"]
            + "\n\n"
            + prompt_yaml["output_format"],
        ),
    ]
)


# ==================================================
# 3. Retrieval RPC (Vector / BM25)
# ==================================================
VECTOR_RPC = os.getenv("VECTOR_RPC", "match_documents_chunks_smk_vector")
BM25_RPC = os.getenv("BM25_RPC", "match_documents_chunks_smk_bm25")


def vector_search_fn(
    question: str,
    top_k: int = 20,
    threshold: float = 0.2,
) -> List[Dict[str, Any]]:
    q_emb = embeddings.embed_query(question)

    res = supabase.rpc(
        VECTOR_RPC,
        {
            "query_embedding": q_emb,
            "match_threshold": threshold,
            "match_count": top_k,
        },
    ).execute()

    docs = res.data or []
    for d in docs:
        d["source"] = "vector"
        d["vector_score"] = float(d.get("score", 0.0))
    return docs


def bm25_search_fn(
    question: str,
    top_k: int = 20,
) -> List[Dict[str, Any]]:
    res = supabase.rpc(
        BM25_RPC,
        {
            "query": question,
            "match_count": top_k,
        },
    ).execute()

    docs = res.data or []
    for d in docs:
        d["source"] = "bm25"
        d["bm25_score"] = float(d.get("score", 0.0))
    return docs


def hybrid_merge(
    vector_docs: List[Dict[str, Any]],
    bm25_docs: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    merged: Dict[str, Dict[str, Any]] = {}

    for d in vector_docs:
        key = str(d["chunk_id"])
        merged[key] = d

    for d in bm25_docs:
        key = str(d["chunk_id"])
        if key in merged:
            merged[key]["source"] = "hybrid(vector+bm25)"
            merged[key]["bm25_score"] = d.get("bm25_score", 0.0)
        else:
            merged[key] = d

    return list(merged.values())


# ==================================================
# 4. Rerank (BGE CrossEncoder)
# ==================================================
def bge_rerank(
    question: str,
    docs: List[Dict[str, Any]],
    k: int = 6,
) -> List[Dict[str, Any]]:
    if not docs:
        return []

    pairs = []
    kept_docs = []

    for d in docs:
        txt = d.get("text", "").strip()
        if not txt:
            continue
        pairs.append((question, txt))
        kept_docs.append(d)

    if not pairs:
        return []

    scores = reranker.predict(pairs)
    for d, s in zip(kept_docs, scores):
        d["rerank_score"] = float(s)

    kept_docs.sort(
        key=lambda x: x.get("rerank_score", 0.0),
        reverse=True,
    )
    return kept_docs[:k]


# ==================================================
# 5. Context ìƒì„± (documents_chunks_smk ìŠ¤í‚¤ë§ˆ ì •í•©)
# ==================================================
def build_contexts(docs: List[Dict[str, Any]]) -> List[str]:
    contexts: List[str] = []

    for d in docs:
        metadata = d.get("metadata") or {}

        parent_section = metadata.get("parent_section", "ì •ë³´ ì—†ìŒ")
        related_section = metadata.get("related_section", "ì •ë³´ ì—†ìŒ")

        pages = d.get("pages")
        pages_str = (
            ", ".join(map(str, pages))
            if isinstance(pages, list)
            else "ì •ë³´ ì—†ìŒ"
        )

        contexts.append(
            f"""
[ê³µê³  ì •ë³´]
- ê³µê³  ë²ˆí˜¸: {d.get("announcement_id")}
- ê³µê³  ì°¨ìˆ˜: {d.get("announcement_round")}
- ê³µê³ ëª…: {d.get("project_name")}
- ì‚¬ì—… ê¸ˆì•¡: {d.get("project_budget")}
- ë°œì£¼ ê¸°ê´€: {d.get("ordering_agency")}
- ê³µê°œ ì¼ì: {d.get("published_at")}
- ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼: {d.get("bid_start_at")}
- ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼: {d.get("bid_end_at")}

[ë¬¸ì„œ ìœ„ì¹˜ ì •ë³´]
- í˜ì´ì§€(pages): {pages_str}
- chunk_index: {d.get("chunk_index")}
- content_type: {d.get("content_type")}

[íŒŒì¼ ì •ë³´]
- íŒŒì¼ëª…: {d.get("source_file")}
- íŒŒì¼í˜•ì‹: {d.get("file_type")}
- ë¬¸ì„œ ê¸¸ì´(length): {d.get("length")}

[ë¬¸ì„œ êµ¬ì¡° ì •ë³´]
- ìƒìœ„ ì„¹ì…˜(parent_section): {parent_section}
- ì—°ê´€ ì„¹ì…˜(related_section): {related_section}

[ê²€ìƒ‰ ì •ë³´]
- ê²€ìƒ‰ ì†ŒìŠ¤: {d.get("source")}
- vector_score: {d.get("vector_score")}
- bm25_score: {d.get("bm25_score")}
- rerank_score: {d.get("rerank_score")}

[ì‚¬ì—…ìš”ì•½]
{d.get("text")}
""".strip()
        )

    return contexts


# ==================================================
# 6. LangSmith Runnable Pipeline
# ==================================================
def step_vector(x: Dict[str, Any]) -> Dict[str, Any]:
    tqdm.write("    [1/5] Vector ê²€ìƒ‰ ì¤‘...")
    vdocs = vector_search_fn(x["question"])
    return {**x, "vector_docs": vdocs}


def step_bm25(x: Dict[str, Any]) -> Dict[str, Any]:
    tqdm.write("    [2/5] BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘...")
    kdocs = bm25_search_fn(x["question"])
    return {**x, "bm25_docs": kdocs}


def step_merge(x: Dict[str, Any]) -> Dict[str, Any]:
    tqdm.write("    [3/5] Hybrid merge ì¤‘...")
    merged = hybrid_merge(
        x.get("vector_docs", []),
        x.get("bm25_docs", []),
    )
    return {**x, "merged_docs": merged}


def step_rerank(x: Dict[str, Any]) -> Dict[str, Any]:
    tqdm.write(f"    [4/5] BGE rerank ì¤‘ ({RERANKER_MODEL})...")
    reranked = bge_rerank(
        x["question"],
        x.get("merged_docs", []),
    )
    return {**x, "reranked_docs": reranked}


def step_answer(x: Dict[str, Any]) -> Dict[str, Any]:
    tqdm.write("    [5/5] LLM ë‹µë³€ ìƒì„± ì¤‘...")
    contexts = build_contexts(x.get("reranked_docs", []))
    ctx_text = "\n\n".join(contexts) if contexts else "ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

    messages = prompt.format_messages(
        question=x["question"],
        contexts=ctx_text,
    )
    answer = llm.invoke(messages)

    return {
        **x,
        "contexts": contexts,
        "answer": answer.content,
    }


rag_pipeline = (
    RunnableLambda(lambda q: {"question": q})
    | RunnableLambda(step_vector)
    | RunnableLambda(step_bm25)
    | RunnableLambda(step_merge)
    | RunnableLambda(step_rerank)
    | RunnableLambda(step_answer)
)


# ==================================================
# 7. ì‹¤í–‰ (Golden Dataset â†’ RAGAS ì…ë ¥)
# ==================================================
if __name__ == "__main__":
    GOLDEN_PATH = BASE_DIR / "src" / "dataset" / "goldendataset.json"
    OUTPUT_PATH = BASE_DIR / "src" / "dataset" / "ragas_inputs.json"

    with open(GOLDEN_PATH, "r", encoding="utf-8") as f:
        golden_data = json.load(f)

    print(
        f"\nğŸš€ Hybrid(Vector+BM25)+BGE Reranker ì‹¤í–‰ "
        f"(ì´ {len(golden_data)}ê°œ)\n"
    )

    results = []

    for item in tqdm(golden_data, desc="Hybrid RAG", unit="sample"):
        qid = item["id"]
        question = item["question"]

        tqdm.write(f"\nâ–¶ [{qid}] ì²˜ë¦¬ ì‹œì‘")
        try:
            out = rag_pipeline.invoke(question)

            results.append(
                {
                    "id": qid,
                    "question": question,
                    "contexts": out["contexts"],
                    "answer": out["answer"],
                    "ground_truth": item.get("ground_truth", ""),
                }
            )

            tqdm.write(f"âœ” [{qid}] ì™„ë£Œ")

        except Exception as e:
            tqdm.write(f"âœ– [{qid}] ì‹¤íŒ¨: {repr(e)}")
            results.append(
                {
                    "id": qid,
                    "question": question,
                    "contexts": [],
                    "answer": "",
                    "ground_truth": item.get("ground_truth", ""),
                    "error": repr(e),
                }
            )

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print("\nâœ… ì™„ë£Œ")
    print(f"ğŸ“„ RAGAS ì…ë ¥ íŒŒì¼: {OUTPUT_PATH}")
