import json
import os
from pathlib import Path
from typing import List, Dict, Any

from dotenv import load_dotenv
import yaml
from supabase import create_client
from tqdm import tqdm

import torch
from sentence_transformers import CrossEncoder

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda


# ==================================================
# 0. í™˜ê²½ ë¡œë“œ + LangSmith ì„¤ì •
# ==================================================
BASE_DIR = Path(__file__).resolve().parents[2]
load_dotenv(BASE_DIR / ".env")

# LangSmith tracing (í•„ìš” env: LANGCHAIN_API_KEY)
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "rfp-rag-eval")

# ==================================================
# 1. Supabase / Embedding / LLM / Reranker
# ==================================================
supabase = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY"),
)

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

llm = ChatOpenAI(
    model=os.getenv("OPENAI_LLM_MODEL", "gpt-4o-mini"),
    temperature=0,
)

# âœ… ìš”ì²­: bge reranker m3 ko
# (ê²€ìƒ‰ ê²°ê³¼ ê¸°ì¤€) dragonkue/bge-reranker-v2-m3-ko :contentReference[oaicite:0]{index=0}
RERANKER_MODEL = os.getenv("RERANKER_MODEL", "dragonkue/bge-reranker-v2-m3-ko")
device = "cuda" if torch.cuda.is_available() else "cpu"
reranker = CrossEncoder(RERANKER_MODEL, device=device)


# ==================================================
# 2. Prompt YAML ë¡œë”©
# ==================================================
PROMPT_PATH = BASE_DIR / "src" / "prompts" / "ragas_template.yaml"
with open(PROMPT_PATH, "r", encoding="utf-8") as f:
    prompt_yaml = yaml.safe_load(f)["prompt"]

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", prompt_yaml["system"]),
        (
            "human",
            prompt_yaml["instructions"]
            + "\n\n"
            + prompt_yaml["context_format"]
            + "\n\n"
            + prompt_yaml["user_prompt"]
            + "\n\n"
            + prompt_yaml["answer_guidelines"]
            + "\n\n"
            + prompt_yaml["output_format"],
        ),
    ]
)


# ==================================================
# 3. Retrieval RPC (Vector / BM25)
# ==================================================
VECTOR_RPC = os.getenv("VECTOR_RPC", "match_documents_chunks_structural_vector")
BM25_RPC = os.getenv("BM25_RPC", "match_documents_chunks_structural_bm25")


def vector_search_fn(question: str, top_k: int = 20, threshold: float = 0.2) -> List[Dict[str, Any]]:
    """Supabase RPCë¡œ vector ê²€ìƒ‰"""
    q_emb = embeddings.embed_query(question)

    res = supabase.rpc(
        VECTOR_RPC,
        {
            "query_embedding": q_emb,
            "match_threshold": threshold,
            "match_count": top_k,
        },
    ).execute()

    docs = res.data or []
    for d in docs:
        d["source"] = "vector"
        d["vector_score"] = float(d.get("score", 0.0))
    return docs


def bm25_search_fn(question: str, top_k: int = 20) -> List[Dict[str, Any]]:
    """Supabase RPCë¡œ FTS(BM25 ìœ ì‚¬) ê²€ìƒ‰"""
    res = supabase.rpc(
        BM25_RPC,
        {
            "query": question,
            "match_count": top_k,
        },
    ).execute()

    docs = res.data or []
    for d in docs:
        d["source"] = "bm25"
        d["bm25_score"] = float(d.get("score", 0.0))
    return docs


def hybrid_merge(vector_docs: List[Dict[str, Any]], bm25_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """chunk_id ê¸°ì¤€ dedup merge"""
    merged: Dict[str, Dict[str, Any]] = {}

    for d in vector_docs:
        key = str(d.get("chunk_id"))
        merged[key] = d

    for d in bm25_docs:
        key = str(d.get("chunk_id"))
        if key in merged:
            # ë‘˜ ë‹¤ ê±¸ë¦° ê²½ìš° source í‘œì‹œ ê°•í™”
            merged[key]["source"] = "hybrid(vector+bm25)"
            merged[key]["bm25_score"] = float(d.get("bm25_score", d.get("score", 0.0)))
        else:
            merged[key] = d

    return list(merged.values())


# ==================================================
# 4. Rerank
# ==================================================
def bge_rerank(question: str, docs: List[Dict[str, Any]], k: int = 6) -> List[Dict[str, Any]]:
    """CrossEncoderë¡œ ì¬ì •ë ¬"""
    if not docs:
        return []

    pairs = []
    kept_docs = []
    for d in docs:
        txt = d.get("text") or ""
        if not txt.strip():
            continue
        pairs.append((question, txt))
        kept_docs.append(d)

    if not pairs:
        return []

    scores = reranker.predict(pairs)
    for d, s in zip(kept_docs, scores):
        d["rerank_score"] = float(s)

    kept_docs.sort(key=lambda x: x.get("rerank_score", 0.0), reverse=True)
    return kept_docs[:k]


# ==================================================
# 5. Context ìƒì„± (ìš”ì²­: í•œê¸€ ì»¬ëŸ¼ëª… í¬í•¨)
# ==================================================
def build_contexts(docs: List[Dict[str, Any]]) -> List[str]:
    contexts: List[str] = []

    for d in docs:
        # âœ… í•µì‹¬: metadata ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        metadata = d.get("metadata") or {}

        parent_section = metadata.get("parent_section", "ì •ë³´ ì—†ìŒ")
        related_section = metadata.get("related_section", "ì •ë³´ ì—†ìŒ")

        contexts.append(
            f"""
[ê³µê³  ì •ë³´]
- ê³µê³  ë²ˆí˜¸: {d.get("announcement_id")}
- ê³µê³  ì°¨ìˆ˜: {d.get("announcement_round")}
- ê³µê³ ëª…: {d.get("project_name")}
- ì‚¬ì—… ê¸ˆì•¡: {d.get("project_budget")}
- ë°œì£¼ ê¸°ê´€: {d.get("ordering_agency")}
- ê³µê°œ ì¼ì: {d.get("published_at")}
- ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼: {d.get("bid_start_at")}
- ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼: {d.get("bid_end_at")}
- íŒŒì¼ëª…: {d.get("source_file")}
- íŒŒì¼í˜•ì‹: {d.get("file_type")}
- ë¬¸ì„œ ê¸¸ì´(length): {d.get("length")}

[ë¬¸ì„œ êµ¬ì¡° ì •ë³´]
- ìƒìœ„ ì„¹ì…˜(parent_section): {parent_section}
- ì—°ê´€ ì„¹ì…˜(related_section): {related_section}

[ê²€ìƒ‰ ì •ë³´]
- ê²€ìƒ‰ ì†ŒìŠ¤: {d.get("source")}
- vector_score: {d.get("vector_score")}
- bm25_score: {d.get("bm25_score")}
- rerank_score: {d.get("rerank_score")}

[ì‚¬ì—…ìš”ì•½]
{d.get("text")}
""".strip()
        )

    return contexts


# ==================================================
# 6. LangSmith ë‹¨ê³„ë³„ Tracingì„ ìœ„í•œ Runnable êµ¬ì„±
# ==================================================
def step_vector(x: Dict[str, Any]) -> Dict[str, Any]:
    q = x["question"]
    tqdm.write(f"    [1/5] Vector ê²€ìƒ‰ ì¤‘...")
    vdocs = vector_search_fn(q, top_k=20, threshold=0.2)
    return {**x, "vector_docs": vdocs}


def step_bm25(x: Dict[str, Any]) -> Dict[str, Any]:
    q = x["question"]
    tqdm.write(f"    [2/5] BM25(ìœ ì‚¬) í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘...")
    kdocs = bm25_search_fn(q, top_k=20)
    return {**x, "bm25_docs": kdocs}


def step_merge(x: Dict[str, Any]) -> Dict[str, Any]:
    tqdm.write(f"    [3/5] Hybrid merge ì¤‘...")
    merged = hybrid_merge(x.get("vector_docs", []), x.get("bm25_docs", []))
    return {**x, "merged_docs": merged}


def step_rerank(x: Dict[str, Any]) -> Dict[str, Any]:
    q = x["question"]
    tqdm.write(f"    [4/5] BGE rerank ì¤‘... (model={RERANKER_MODEL}, device={device})")
    reranked = bge_rerank(q, x.get("merged_docs", []), k=6)
    return {**x, "reranked_docs": reranked}


def step_context_and_answer(x: Dict[str, Any]) -> Dict[str, Any]:
    q = x["question"]
    tqdm.write(f"    [5/5] LLM ë‹µë³€ ìƒì„± ì¤‘...")
    contexts = build_contexts(x.get("reranked_docs", []))
    ctx_text = "\n\n".join(contexts) if contexts else "ê´€ë ¨ ë¬¸ì„œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    messages = prompt.format_messages(question=q, contexts=ctx_text)
    answer = llm.invoke(messages)
    return {**x, "contexts": contexts, "answer": answer.content}


rag_pipeline = (
    RunnableLambda(lambda q: {"question": q}).with_config(run_name="00_User_Query")
    | RunnableLambda(step_vector).with_config(run_name="01_Vector_Retrieval")
    | RunnableLambda(step_bm25).with_config(run_name="02_BM25_Retrieval")
    | RunnableLambda(step_merge).with_config(run_name="03_Hybrid_Merge")
    | RunnableLambda(step_rerank).with_config(run_name="04_BGE_Reranking")
    | RunnableLambda(step_context_and_answer).with_config(run_name="05_Context_Prompt_LLM")
).with_config(run_name="RAG_Pipeline_Hybrid_BGE")


# ==================================================
# 7. ì‹¤í–‰
# ==================================================
if __name__ == "__main__":
    GOLDEN_PATH = BASE_DIR / "src" / "dataset" / "goldendataset.json"
    OUTPUT_PATH = BASE_DIR / "src" / "dataset" / "ragas_inputs.json"

    with open(GOLDEN_PATH, "r", encoding="utf-8") as f:
        golden_data = json.load(f)

    print(f"\nğŸš€ Hybrid(Vector+BM25) + BGE Reranker + LangSmith tracing ì‹œì‘ (ì´ {len(golden_data)}ê°œ)\n")
    results = []

    for item in tqdm(golden_data, desc="Hybrid RAG tracing", unit="sample"):
        qid = item["id"]
        question = item["question"]

        tqdm.write(f"\nâ–¶ [{qid}] ì²˜ë¦¬ ì‹œì‘")
        try:
            out = rag_pipeline.invoke(
                question,
                config={"tags": [qid]},  # LangSmithì—ì„œ idë¡œ í•„í„°ë§ ê°€ëŠ¥
            )

            results.append(
                {
                    "id": qid,
                    "question": question,
                    "contexts": out.get("contexts", []),
                    "answer": out.get("answer", ""),
                    "ground_truth": item.get("ground_truth", ""),
                }
            )

            tqdm.write(f"âœ” [{qid}] ì™„ë£Œ")

        except Exception as e:
            tqdm.write(f"âœ– [{qid}] ì‹¤íŒ¨: {repr(e)}")
            # ì‹¤íŒ¨ ìƒ˜í”Œë„ ë‚¨ê¸°ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ì²˜ëŸ¼ ì €ì¥
            results.append(
                {
                    "id": qid,
                    "question": question,
                    "contexts": [],
                    "answer": "",
                    "ground_truth": item.get("ground_truth", ""),
                    "error": repr(e),
                }
            )

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì™„ë£Œ: Hybrid RAG + BGE reranker + RAGAS ì…ë ¥ ìƒì„±")
    print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {OUTPUT_PATH}")
