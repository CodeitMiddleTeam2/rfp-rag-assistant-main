import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

#==============================================
# í”„ë¡œê·¸ë¨ëª…: train_rfp.py
# í´ë”ìœ„ì¹˜: src/post_train/train_rfp.py
# í”„ë¡œê·¸ë¨ ì„¤ëª…: unsloth í—ˆë¸Œì—ì„œ base ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì‚¬ì „ í•™ìŠµ ì‹œí‚¤ëŠ” í”„ë¡œê·¸ë¨
# ì‘ì„±ì´ë ¥: 25.12.22 í•œìƒì¤€ ìµœì´ˆ ì‘ì„±
#===============================================

model_name = "unsloth/Qwen3-8B" 

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)

# í•™ìŠµ ì„¤ì • (QLoRA)
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
)

# [3. ë°ì´í„° í¬ë§·íŒ… í•¨ìˆ˜]
# ì‚¬ìš©ìë‹˜ì˜ JSONL êµ¬ì¡°(question, contexts, answer)ì— ë§ì¶° í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
alpaca_prompt = """ë‹¹ì‹ ì€ B2G ì…ì°° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µí•˜ì„¸ìš”.

### ì§ˆë¬¸:
{}

### ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:
{}

### ë‹µë³€:
{}"""

def formatting_prompts_func(examples):
    instructions = examples["question"]
    # contexts ë¦¬ìŠ¤íŠ¸ ë‚´ì˜ ê° í•­ëª©ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ í•©ì³ ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
    contexts     = ["\n".join(c) for c in examples["contexts"]] 
    outputs      = examples["answer"]
    texts = []
    for instruction, context, output in zip(instructions, contexts, outputs):
        text = alpaca_prompt.format(instruction, context, output) + tokenizer.eos_token
        texts.append(text)
    return { "text" : texts, }

# [4. ë°ì´í„°ì…‹ ë¡œë“œ ë° ë§¤í•‘]
# train_sft.jsonl íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ í¬ë§·íŒ…ì„ ì ìš©í•©ë‹ˆë‹¤.
dataset = load_dataset("json", data_files="src/post_train/train_sft.jsonl", split="train")
dataset = dataset.map(formatting_prompts_func, batched = True)

# [5. í•™ìŠµ íŒŒë¼ë¯¸í„° ì¡°ì • (450ê°œ ë°ì´í„° ìµœì í™”)]
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = 2048,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4, # ì´ ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 8
        
        # ë°ì´í„°ê°€ 450ê°œì´ë¯€ë¡œ 3ë²ˆ ë°˜ë³µ(3 Epochs) í•™ìŠµì„ ì¶”ì²œí•©ë‹ˆë‹¤.
        # ì•½ 170~180 Steps ì •ë„ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤.
        num_train_epochs = 3, 
        
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        output_dir = "outputs",
        save_strategy = "no",
    ),
)

# [6. ì‹¤í–‰ ë° ì €ì¥]
print("ğŸš€ ì´ 450ê°œì˜ ë°ì´í„°ë¡œ RFP íŠ¹í™” í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()

# GGUFë¡œ ì¦‰ì‹œ ë³€í™˜ ë° ì €ì¥
model.save_pretrained_gguf("final_model_gguf", tokenizer, quantization_method = "q4_k_m")