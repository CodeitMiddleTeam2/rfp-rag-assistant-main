from unsloth import FastLanguageModel

#==============================================
# í”„ë¡œê·¸ë¨ëª…: convert_gguf.py
# í´ë”ìœ„ì¹˜: src/post_train/convert_gguf.py
# í”„ë¡œê·¸ë¨ ì„¤ëª…: ì‚¬ì „í•™ìŠµ ì‹œí‚¨ ëª¨ë¸ì„ gguf íŒŒì¼ë¡œ ë³€í™˜í•˜ëŠ” í”„ë¡œê·¸ë¨
# ì‘ì„±ì´ë ¥: 25.12.22 í•œìƒì¤€ ìµœì´ˆ ì‘ì„±
#===============================================

# 1. ë°©ê¸ˆ í•™ìŠµì„ ë§ˆì¹˜ê³  ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# (final_model_gguf í´ë”ì— 16-bit ìƒíƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤)
print("ğŸ“‚ ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "./final_model_gguf", 
    load_in_4bit = False, # ë³€í™˜ì„ ìœ„í•´ ì›ë³¸(16bit) ì •ë°€ë„ë¡œ ë¡œë“œ
)

# 2. GGUF ë³€í™˜ ë‹¤ì‹œ ì‹œë„
print("ğŸ’¾ GGUF ë³€í™˜ì„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
model.save_pretrained_gguf(
    "final_model_gguf", 
    tokenizer, 
    quantization_method = "q4_k_m"
)
print("âœ… ë³€í™˜ ì„±ê³µ!")