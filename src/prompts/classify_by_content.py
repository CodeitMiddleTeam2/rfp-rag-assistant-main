import os
import pandas as pd
import olefile
import zlib
import struct
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path
from pypdf import PdfReader

# ===================================================================
# í”„ë¡œê·¸ë¨ ëª…: classify_by_content.py
# ìœ„ì¹˜: src/prompt/classify_by_content.py
# í”„ë¡œê·¸ë¨ ì„¤ëª…: ì›ë³¸ pdf/hwp ë°ì´í„°ì˜ ë³¸ë¬¸ì„ ë³´ê³  ë„ë©”ì¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜, ì´í›„ csvíŒŒì¼ë¡œ ê²°ê³¼ë¬¼ ì €ì¥
# ì‘ì„±ì´ë ¥ 
#         25.12.17 í•œìƒì¤€ ìµœì´ˆ ì‘ì„±
# ===================================================================

# --- ì„¤ì • ---
load_dotenv()
current_dir = Path(__file__).resolve().parent
PROJECT_ROOT = current_dir.parent.parent
DATA_DIR = PROJECT_ROOT / "data/rfp_data"
OUTPUT_FILE = "rfp_classification_precise.csv"

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ----------------------------------------------------------------
# [í•µì‹¬] HWP ë³¸ë¬¸ ê°•ì œ ì¶”ì¶œ í•¨ìˆ˜ (Deep Extraction)
# ----------------------------------------------------------------
def extract_hwp_text_deep(file_path):
    try:
        if not olefile.isOleFile(file_path):
            return "HWP í¬ë§· ì•„ë‹˜ (HWPXì¼ ê°€ëŠ¥ì„± ìˆìŒ)"

        f = olefile.OleFileIO(file_path)
        dirs = f.listdir()
        
        # 1. BodyText ì„¹ì…˜ ì°¾ê¸° (ë³¸ë¬¸ ë‚´ìš©ì´ ë‹´ê¸´ ê³³)
        # ë³´í†µ BodyText/Section0, Section1... í˜•íƒœë¡œ ì¡´ì¬í•¨
        body_sections = [d for d in dirs if d[0] == "BodyText"]
        
        extracted_text = ""
        
        for section in body_sections:
            stream = f.openstream(section)
            data = stream.read()
            
            # 2. Zlib ì••ì¶• í•´ì œ (HWP ë³¸ë¬¸ì€ ì••ì¶•ë˜ì–´ ìˆìŒ)
            try:
                # -15: raw stream (header ì—†ì´ ì••ì¶•ëœ ë°ì´í„° ì²˜ë¦¬)
                decompressed = zlib.decompress(data, -15) 
            except:
                try:
                    decompressed = zlib.decompress(data)
                except:
                    continue # ì••ì¶• í•´ì œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ
            
            # 3. í…ìŠ¤íŠ¸ ë³€í™˜ (UTF-16LE)
            # HWP í…ìŠ¤íŠ¸ëŠ” ìœ ë‹ˆì½”ë“œ(UTF-16 Little Endian)ë¡œ ì €ì¥ë¨
            section_text = decompressed.decode('utf-16le', errors='ignore')
            
            # 4. ì œì–´ ë¬¸ì ë° ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ì •ì œ)
            # HWP íŠ¹ìˆ˜ë¬¸ìë‚˜ í‘œ ì œì–´ë¬¸ì ë“±ì´ ì„ì—¬ìˆìœ¼ë¯€ë¡œ, ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
            clean_text = "".join([c for c in section_text if c.isprintable() or c in ['\n', ' ', '\t']])
            extracted_text += clean_text + "\n"
            
            # ì•ë¶€ë¶„ 4000ìë§Œ ëª¨ìœ¼ë©´ ì¶©ë¶„í•¨ (ë©”íƒ€ë°ì´í„° ë¶„ì„ìš©)
            if len(extracted_text) > 4000:
                break
                
        f.close()
        
        if not extracted_text.strip():
            return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì•”í˜¸í™” ë˜ëŠ” ë¹ˆ ë¬¸ì„œ)"
            
        return extracted_text[:4000]

    except Exception as e:
        return f"Error: {str(e)}"

# --- PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
def extract_text_from_pdf(file_path):
    try:
        reader = PdfReader(file_path)
        text = ""
        # ë„‰ë„‰í•˜ê²Œ 7í˜ì´ì§€ê¹Œì§€ ì½ê¸°
        for page in reader.pages[:min(7, len(reader.pages))]:
            text += page.extract_text() or ""
        return text[:4000]
    except Exception as e:
        return ""

# --- LLM ë¶„ë¥˜ê¸° (ë™ì¼) ---
def classify_file_content(filename, content):
    if len(content) < 50:
        return "íŒë…ë¶ˆê°€"

    prompt = f"""
    ë‹¹ì‹ ì€ í”„ë¡œì íŠ¸ ì œì•ˆìš”ì²­ì„œ(RFP) ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì œê³µëœ [íŒŒì¼ëª…]ê³¼ [ë¬¸ì„œ ë‚´ìš©]ì„ ë¶„ì„í•˜ì—¬, ì´ ì‚¬ì—…ì˜ ì„±ê²©ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.

    [ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬]
    1. IT_ì •ë³´í™”: ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ, ì‹œìŠ¤í…œ êµ¬ì¶•, í†µì‹ ë§, ì „ì‚° ì¥ë¹„(ì„œë²„/PC) ë„ì… ë“±
    2. ê³µì‚¬_ì‹œì„¤: ê±´ì¶•, í† ëª©, ì¸í…Œë¦¬ì–´, ì „ê¸°/ì†Œë°© ê³µì‚¬, ì‹œì„¤ë¬¼ ì„¤ì¹˜ ë“±
    3. ë¬¼í’ˆ_êµ¬ë§¤: ê°€êµ¬, ì°¨ëŸ‰, ì˜ì•½í’ˆ, ì¼ë°˜ ë¹„í’ˆ, ê¸°ìì¬ ë‹¨ìˆœ êµ¬ë§¤ (IT ì¥ë¹„ ì œì™¸)
    4. ìš©ì—­_ì¼ë°˜: í•™ìˆ  ì—°êµ¬, í–‰ì‚¬ ëŒ€í–‰, ì²­ì†Œ/ê²½ë¹„, í™ë³´ë¬¼ ì œì‘, ë²ˆì—­, ë‹¨ìˆœ ì¸ë ¥ íŒŒê²¬ ë“±

    [íŒŒì¼ëª…]: {filename}
    [ë‚´ìš©]: {content}

    [ì§€ì¹¨]
    1. ë‚´ìš©ì´ ë³µí•©ì ì¼ ê²½ìš°, ì˜ˆì‚° ë¹„ì¤‘ì´ ë” í¬ê±°ë‚˜ ì£¼ëœ ê³¼ì—…ì´ë¼ê³  íŒë‹¨ë˜ëŠ” ìª½ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    2. ì¶œë ¥ì€ ì˜¤ì§ ìœ„ 4ê°œ ì¤‘ í•´ë‹¹í•˜ëŠ” 'ì¹´í…Œê³ ë¦¬ëª…' í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”. (ì˜ˆ: ê³µì‚¬_ì‹œì„¤)
    """

    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(('.hwp', '.pdf'))]
    results = []
    
    print(f"ğŸ•µï¸ ì´ {len(files)}ê°œ íŒŒì¼ì˜ ì‹¬ì¸µ ë¶„ì„(Deep Analysis)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    for i, fname in enumerate(files):
        file_path = DATA_DIR / fname
        content = ""
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if fname.lower().endswith('.hwp'):
            content = extract_hwp_text_deep(file_path)
        elif fname.lower().endswith('.pdf'):
            content = extract_text_from_pdf(file_path)
        
        # 2. ë‚´ìš© í™•ì¸ ë° ë¶„ë¥˜
        if len(content) < 20 or "Error" in content or "ì‹¤íŒ¨" in content:
            print(f"[{i+1}/{len(files)}] âš ï¸ {fname}: {content[:30]}...")
            category = "íŒë…ë¶ˆê°€"
        else:
            # LLM ë¶„ë¥˜ ì‹¤í–‰
            category = classify_file_content(fname, content)
            print(f"[{i+1}/{len(files)}] âœ… {fname} -> {category}")

        results.append({
            "FileName": fname, 
            "Category": category, 
            "ExtractedSnippet": content[:100].replace('\n', ' ')
        })

    # ì €ì¥
    df = pd.DataFrame(results)
    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ! '{OUTPUT_FILE}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(df['Category'].value_counts())