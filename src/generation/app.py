import streamlit as st
import pandas as pd
import os
import sys
from dotenv import load_dotenv

#==============================================
# í”„ë¡œê·¸ë¨ëª…: app.py
# í´ë”ìœ„ì¹˜: src/generation/app.py
# í”„ë¡œê·¸ë¨ ì„¤ëª…: RAG ê¸°ë°˜ RFP ë¶„ì„ í”Œë«í¼ (DB + Rerank + Local LLM)
# ì‘ì„±ì´ë ¥: 2025.12.19 í•œìƒì¤€ ìµœì´ˆ ì‘ì„±
#          12.21 ìˆ˜ì • : í•œìƒì¤€ ëŒ€ë¶„ë¥˜ ì¢…í•©ëª¨ë“œ ì¶”ê°€
#          12.23 ìˆ˜ì • : í•œìƒì¤€ DB ì—°ë™ ì½”ë“œ ì¶”ê°€
#          12.24 ìˆ˜ì • : í•œìƒì¤€ rerank ì¶”ê°€
#          12.29 ìˆ˜ì • : src/rag/db.py rerank_model.py embedding_model.py ë³‘í•©
#===============================================

# [1. í™˜ê²½ ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì •]
load_dotenv()

current_file = os.path.abspath(__file__)
generation_dir = os.path.dirname(current_file)
src_dir = os.path.dirname(generation_dir)
root_dir = os.path.dirname(src_dir)
model_path = os.path.join(root_dir, "unsloth.Q4_K_M.gguf")

if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

# [2. ëª¨ë“ˆ ì„í¬íŠ¸]
try:
    from src.prompts.RAGPromptBuilder import RAGPromptBuilder
    from src.generation.model_manager import ModelManager
    from src.rag.embed.embedding_model import EmbeddingModel
    from src.rag.rerank.rerank_model import RerankModel
except ImportError as e:
    st.error(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    st.stop()

# [3. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜]
def load_hierarchical_data():
    csv_path = os.path.join(root_dir, 'final_classification_hierarchy.csv')
    if os.path.exists(csv_path):
        return pd.read_csv(csv_path)
    else:
        st.error("ğŸš¨ ê³„ì¸µ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

def main():
    st.set_page_config(page_title="RFP Intelligence Platform", layout="wide", page_icon="ğŸ¢")

    # ë°ì´í„° ë¡œë“œ (ì‚¬ì´ë“œë°” í•„í„°ìš©)
    df = load_hierarchical_data()
    if df is None: return

    # âœ… ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
    # ModelManagerëŠ” ë‚´ë¶€ ìºì‹±ë˜ë¯€ë¡œ ë§¤ë²ˆ í˜¸ì¶œí•´ë„ ì•ˆì „í•¨
    model_manager = ModelManager(local_model_path=model_path)

    # âœ… Advanced RAG ëª¨ë“ˆ ì´ˆê¸°í™”
    try:
        embedding_model = EmbeddingModel("text-embedding-3-small")
        rerank_model = RerankModel("dragonkue/bge-reranker-v2-m3-ko") # L4 GPU ìë™ ì‚¬ìš©ë¨
    except Exception as e:
        st.error(f"âŒ RAG ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()

    # í”„ë¡¬í”„íŠ¸ ë¹Œë” ì´ˆê¸°í™”
    try:
        prompt_dir = os.path.join(root_dir, 'src', 'prompts')
        builder = RAGPromptBuilder(prompt_dir)
    except:
        st.warning("âš ï¸ í”„ë¡¬í”„íŠ¸ ë¹Œë” ì´ˆê¸°í™” ì‹¤íŒ¨. ê¸°ë³¸ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        builder = None

    st.title("ğŸ¢ B2G ì…ì°° ë¶„ì„ í”Œë«í¼: ê³„ì¸µí˜• íƒìƒ‰ ëª¨ë“œ")
    st.markdown("ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì‚¬ì—…ì„ íƒìƒ‰í•˜ê³ , **DB ê¸°ë°˜ì˜ ì •ë°€ RAG ë¶„ì„**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")

    # ---------------------------------------------------------
    # [Sidebar] ì„¤ì • ë° í•„í„°
    # ---------------------------------------------------------
    with st.sidebar:
        st.header("âš™ï¸ ëª¨ë¸ ì„¤ì •")
        model_source = st.radio("ì‚¬ìš© ëª¨ë¸", ("OpenAI API (GPT-5-nano)", "Local Model (Qwen-3-8B)"), index=0)
        
        openai_client = None
        local_llm = None
        source_key = "openai"

        if "OpenAI" in model_source:
            source_key = "openai"
            if not os.getenv("OPENAI_API_KEY"):
                st.error("ğŸš¨ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                openai_client = model_manager.get_openai_client()
                st.success("ğŸŸ¢ API Ready")
        else:
            source_key = "local"
            with st.spinner("ğŸš€ ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì¤‘..."):
                # âœ… ë§¤ë‹ˆì €ë¥¼ í†µí•´ ëª¨ë¸ ë¡œë“œ (ë‚´ë¶€ì ìœ¼ë¡œ ìºì‹±ë¨)
                local_llm = model_manager.load_local_model()

            if local_llm: 
                st.success("ğŸŸ¢ Local Model Ready")
            else:
                st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ê²½ë¡œ í™•ì¸: {model_path}")

        st.divider()
        st.header("ğŸ“‚ íƒìƒ‰ í•„í„°")

        # --- Depth 1: ëŒ€ë¶„ë¥˜ ---
        d1_options = ["ğŸ” ì „ì²´ ë°ì´í„° (All RFPs)"] + sorted(df['Depth_1'].unique().tolist())
        selected_d1 = st.selectbox("1ë‹¨ê³„: ëŒ€ë¶„ë¥˜", d1_options)

        display_title = ""
        # âš ï¸ í•„í„°ë§ ë¡œì§: DB ê²€ìƒ‰ì„ ìœ„í•´ 'ì„ íƒëœ ì‚¬ì—…ëª…'ì„ ì¶”ì í•´ì•¼ í•¨
        target_project_name_for_db = "%" # ê¸°ë³¸ê°’: ì „ì²´ ê²€ìƒ‰

        if selected_d1 == "ğŸ” ì „ì²´ ë°ì´í„° (All RFPs)":
            display_title = "ì „ì²´ RFP ë°ì´í„° ì¢…í•© ë¶„ì„"
            selected_d2 = None
            selected_project = None
        else:
            # --- Depth 2: ì¤‘ë¶„ë¥˜ ---
            d2_options = ["ğŸ“‚ í•´ë‹¹ ëŒ€ë¶„ë¥˜ ì „ì²´ ì¢…í•©"] + sorted(df[df['Depth_1'] == selected_d1]['Depth_2'].unique().tolist())
            selected_d2 = st.selectbox("2ë‹¨ê³„: ì¤‘ë¶„ë¥˜", d2_options)

            if selected_d2 == "ğŸ“‚ í•´ë‹¹ ëŒ€ë¶„ë¥˜ ì „ì²´ ì¢…í•©":
                display_title = f"[{selected_d1}] ì¹´í…Œê³ ë¦¬ ì „ì²´ ë¶„ì„"
            else:
                # --- Depth 3: í”„ë¡œì íŠ¸ ---
                projects_in_cat = df[(df['Depth_1'] == selected_d1) & (df['Depth_2'] == selected_d2)]
                proj_options = ["ğŸ í•´ë‹¹ ì¤‘ë¶„ë¥˜ ì „ì²´ ì¢…í•©"] + sorted(projects_in_cat['ì‚¬ì—…ëª…'].tolist())
                selected_project = st.selectbox("3ë‹¨ê³„: ìƒì„¸ ì‚¬ì—…", proj_options)

                if selected_project == "ğŸ í•´ë‹¹ ì¤‘ë¶„ë¥˜ ì „ì²´ ì¢…í•©":
                    display_title = f"[{selected_d2}] í•˜ìœ„ ì‚¬ì—… ì „ì²´ ë¶„ì„"
                else:
                    display_title = selected_project
                    target_project_name_for_db = selected_project # âœ… íŠ¹ì • ì‚¬ì—… ì„ íƒ ì‹œ í•„í„° ì ìš©

    # ---------------------------------------------------------
    # [Main] UI ë ˆì´ì•„ì›ƒ
    # ---------------------------------------------------------
    
    col_info, col_chat = st.columns([1, 1.5])

    with col_info:
        st.subheader(f"ğŸ“Š {display_title}")

        st.info("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ DBì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ì•„ ë‹µë³€í•©ë‹ˆë‹¤.")
        st.markdown(f"**í˜„ì¬ ê²€ìƒ‰ í•„í„°:** `{target_project_name_for_db if target_project_name_for_db != '%' else "ì „ì²´ ë²”ìœ„"}`")

    with col_chat:
        st.subheader("ğŸ’¬ AI ì»¨ì„¤í„´íŠ¸ ì§ˆì˜ì‘ë‹µ")

        chat_container = st.container(height=600)
        
        with chat_container:
            if "messages" not in st.session_state:
                st.session_state.messages = []

            # ì±„íŒ… ë¡œê·¸ ì¶œë ¥
            for msg in st.session_state.messages:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])

        # ì§ˆë¬¸ ì…ë ¥
        if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            st.session_state.messages.append({"role": "user", "content": query})

            with chat_container:
                with st.chat_message("user"):
                    st.markdown(query)

                # ë‹µë³€ ìƒì„±
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    message_placeholder.markdown("â³ DB ê²€ìƒ‰ ì§„í–‰ ì¤‘...")

                    try:
                        # âœ… [ìˆ˜ì • 1] DB ê²€ìƒ‰ í˜¸ì¶œ (Threshold ì„¤ì •)
                        # í•„í„° ê¸°ëŠ¥ì´ ì—†ëŠ” í•¨ìˆ˜ì´ë¯€ë¡œ, ì¼ë‹¨ ë„‰ë„‰í•˜ê²Œ(30~50ê°œ) ê°€ì ¸ì˜µë‹ˆë‹¤.
                        initial_results = embedding_model.search(
                            query=query, 
                            result_count=40, # í•„í„°ë§ì„ ìœ„í•´ ë„‰ë„‰íˆ ì¡°íšŒ
                            threshold=0.3    # ìœ ì‚¬ë„ 0.3 ì´ìƒë§Œ
                        )

                        for doc in initial_results:
                            if 'text' in doc:
                                doc['content'] = doc['text']
                        
                        # âœ… [ìˆ˜ì • 2] íŒŒì´ì¬ ë ˆë²¨ì—ì„œ í•„í„°ë§ (DB í•¨ìˆ˜ê°€ ì§€ì› ì•ˆ í•˜ë¯€ë¡œ ìˆ˜ë™ ì²˜ë¦¬)
                        filtered_results = []

                        if target_project_name_for_db == "%":
                            filtered_results = initial_results
                        else:
                            for doc in initial_results:
                                # 1. DB í…Œì´ë¸”ì˜ ì»¬ëŸ¼('project_name') ì§ì ‘ í™•ì¸ (ê°€ì¥ ì •í™•)
                                p_name = doc.get('project_name')
                                
                                # 2. í˜¹ì‹œ ëª°ë¼ ë©”íƒ€ë°ì´í„° ì•ˆìª½ë„ í™•ì¸ (ì´ì „ í˜¸í™˜ì„±)
                                if not p_name:
                                    p_name = doc.get('metadata', {}).get('project_name')

                                # 3. ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•œ ì‚¬ì—…ëª…ê³¼ ë¹„êµ
                                # (DBì—ëŠ” ë„ì–´ì“°ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê³µë°± ì œê±° í›„ ë¹„êµí•˜ëŠ” ê²Œ ì•ˆì „í•  ìˆ˜ ìˆìŒ)
                                if p_name and p_name == target_project_name_for_db:
                                    filtered_results.append(doc)

                        # (ë””ë²„ê¹…ìš©) í•„í„°ë§ ì „í›„ ê°œìˆ˜ í™•ì¸
                        st.write(f"ê²€ìƒ‰ëœ {len(initial_results)}ê°œ ì¤‘ '{target_project_name_for_db}' ê´€ë ¨ ë¬¸ì„œ {len(filtered_results)}ê°œ í•„í„°ë§ ë¨")

                        retrieval_results = filtered_results

                        if not retrieval_results:
                            combined_context = "ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        else:
                            # 2. Reranking (ìƒìœ„ 3ê°œ)
                            reranked_result_obj = rerank_model.rerank(
                                query, 
                                retrieval_results, 
                                top_k=3
                            )
                            combined_context = reranked_result_obj.content

                            # [ë””ë²„ê¹…] Rerank ì ìˆ˜ ë° ë©”íƒ€ë°ì´í„° í™•ì¸
                            with st.expander("ğŸ” Rerank ê²°ê³¼ ìƒì„¸ ë³´ê¸°"):
                                st.text(combined_context)

                        # âœ… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
                        if builder:
                            final_messages = builder.build_messages(
                                category=selected_d1 if selected_d1 else "General",
                                title=display_title,
                                context=combined_context,
                                history=st.session_state.messages[:-1],
                                query=query
                            )
                        else:
                            # Fallback
                            final_messages = [
                                {"role": "system", "content": "ë‹¹ì‹ ì€ ì…ì°° ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                                {"role": "user", "content": f"ì°¸ê³ ë¬¸ì„œ:\n{combined_context}\n\nì§ˆë¬¸: {query}"}
                            ]

                        # âœ… ë‹µë³€ ìƒì„±
                        message_placeholder.markdown("â³ ë‹µë³€ ìƒì„± ì¤‘...")
                        response_text = model_manager.generate_response(
                            messages=final_messages,
                            source=source_key,
                            local_llm=local_llm,
                            openai_client=openai_client
                        )
                        
                        message_placeholder.markdown(response_text)
                        st.session_state.messages.append({"role": "assistant", "content": response_text})

                    except Exception as e:
                        if "CUDA out of memory" in str(e):
                            st.error("ğŸš¨ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                            model_manager.clear_gpu_memory() # ğŸ‘ˆ ë©”ëª¨ë¦¬ ì²­ì†Œ ì‹¤í–‰
                            st.stop()
                        message_placeholder.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()