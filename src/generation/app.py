import streamlit as st
import pandas as pd
import os
import sys
from dotenv import load_dotenv

# [1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ]
load_dotenv()

# [2. ê²½ë¡œ ì„¤ì • ë° ëª¨ë“ˆ ì„í¬íŠ¸]
current_file = os.path.abspath(__file__)
generation_dir = os.path.dirname(current_file)
src_dir = os.path.dirname(generation_dir)
root_dir = os.path.dirname(src_dir)
model_path = os.path.join(root_dir, "unsloth.Q4_K_M.gguf")

if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

# ğŸ’¡ ì•ˆì „í•œ ì„í¬íŠ¸ ê´€ë¦¬
try:
    from src.prompts.RAGPromptBuilder import RAGPromptBuilder
    from src.generation.model_manager import ModelManager
    from src.generation.supabase_manager import SupabaseManager
except ImportError as e:
    st.error(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    st.stop()

# [2. ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸°í™”]
def load_hierarchical_data():
    csv_path = os.path.join(root_dir, 'final_classification_hierarchy.csv')
    if os.path.exists(csv_path):
        return pd.read_csv(csv_path)
    else:
        st.error("ğŸš¨ ê³„ì¸µ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

def main():
    st.set_page_config(page_title="RFP Intelligence Platform", layout="wide", page_icon="ğŸ¢")
    df = load_hierarchical_data()
    if df is None: return

    # âœ… [ê°ì²´ ìƒì„±] ModelManager ì¸ìŠ¤í„´ìŠ¤í™”
    # ì¸ìŠ¤í„´ìŠ¤ëŠ” ë§¤ ì‹¤í–‰ë§ˆë‹¤ ìƒˆë¡œ ìƒì„±ë˜ì§€ë§Œ, ë‚´ë¶€ì—ì„œ í˜¸ì¶œí•˜ëŠ”
    # _load_llama_cpp_model í•¨ìˆ˜ê°€ ìºì‹±ë˜ì–´ ìˆì–´ ëª¨ë¸ì€ 1ë²ˆë§Œ ë¡œë“œë©ë‹ˆë‹¤.
    model_manager = ModelManager(local_model_path=model_path)
    db_manager = SupabaseManager()

    try:
        prompt_dir = os.path.join(root_dir, 'src', 'prompts')
        builder = RAGPromptBuilder(prompt_dir)
    except:
        st.warning("âš ï¸ RAGPromptBuilderë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ì§€í•©ë‹ˆë‹¤.")
        builder = None

    st.title("ğŸ¢ B2G ì…ì°° ë¶„ì„ í”Œë«í¼: ê³„ì¸µí˜• íƒìƒ‰ ëª¨ë“œ")
    st.markdown("ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì‚¬ì—…ì„ íƒìƒ‰í•˜ê³ , **ì—¬ëŸ¬ ì‚¬ì—…ì„ ë™ì‹œì— ë¹„êµ/ë¶„ì„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ---------------------------------------------------------
    # [Sidebar] ì„¤ì • ë° í•„í„°
    # ---------------------------------------------------------
    with st.sidebar:
        st.header("âš™ï¸ ëª¨ë¸ ì„¤ì •")
        model_source = st.radio("ì‚¬ìš© ëª¨ë¸", ("OpenAI API (GPT-5-mini)", "Local Model (Qwen-3-8B)"), index=0)
        
        openai_client = None
        local_llm = None

        if "OpenAI" in model_source:
            source_key = "openai"
            # âœ… API Key ê²€ì¦ì„ ì—¬ê¸°ì„œ ìˆ˜í–‰ (ë¡œì»¬ ìœ ì €ëŠ” í†µê³¼ ê°€ëŠ¥)
            if not os.getenv("OPENAI_API_KEY"):
                st.error("ğŸš¨ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                openai_client = model_manager.get_openai_client()
                st.success("ğŸŸ¢ API Ready")
        else:
            source_key = "local"
            with st.spinner("ğŸš€ ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì¤‘..."):
                # âœ… ë§¤ë‹ˆì €ë¥¼ í†µí•´ ëª¨ë¸ ë¡œë“œ (ë‚´ë¶€ì ìœ¼ë¡œ ìºì‹±ë¨)
                local_llm = model_manager.load_local_model()
            if local_llm: 
                st.success("ğŸŸ¢ Local Model Ready")
            else:
                st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ê²½ë¡œ í™•ì¸: {model_path}")

        st.divider()

        st.header("ğŸ“‚ íƒìƒ‰ í•„í„°")

        # --- Depth 1: ëŒ€ë¶„ë¥˜ ---
        d1_options = ["ğŸ” ì „ì²´ ë°ì´í„° (All RFPs)"] + sorted(df['Depth_1'].unique().tolist())
        selected_d1 = st.selectbox("1ë‹¨ê³„: ëŒ€ë¶„ë¥˜", d1_options)

        target_rows = pd.DataFrame() # ë¶„ì„ ëŒ€ìƒ ë°ì´í„°
        display_title = ""

        if selected_d1 == "ğŸ” ì „ì²´ ë°ì´í„° (All RFPs)":
            # ì „ì²´ ëª¨ë“œ: í•˜ìœ„ ì˜µì…˜ ë¹„í™œì„±í™”
            target_rows = df
            display_title = "ì „ì²´ RFP ë°ì´í„° ì¢…í•© ë¶„ì„"
            st.info("âš ï¸ ì „ì²´ ë¬¸ì„œëŠ” ì–‘ì´ ë§ì•„ ë¶„ì„ì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            selected_d2 = None
            selected_project = None
        else:
            # --- Depth 2: ì¤‘ë¶„ë¥˜ ---
            d2_options = ["ğŸ“‚ í•´ë‹¹ ëŒ€ë¶„ë¥˜ ì „ì²´ ì¢…í•©"] + sorted(df[df['Depth_1'] == selected_d1]['Depth_2'].unique().tolist())
            selected_d2 = st.selectbox("2ë‹¨ê³„: ì¤‘ë¶„ë¥˜", d2_options)

            if selected_d2 == "ğŸ“‚ í•´ë‹¹ ëŒ€ë¶„ë¥˜ ì „ì²´ ì¢…í•©":
                target_rows = df[df['Depth_1'] == selected_d1]
                display_title = f"[{selected_d1}] ì¹´í…Œê³ ë¦¬ ì „ì²´ ë¶„ì„"
                selected_project = None
            else:
                # --- Depth 3: í”„ë¡œì íŠ¸ ---
                projects_in_cat = df[(df['Depth_1'] == selected_d1) & (df['Depth_2'] == selected_d2)]
                proj_options = ["ğŸ í•´ë‹¹ ì¤‘ë¶„ë¥˜ ì „ì²´ ì¢…í•©"] + sorted(projects_in_cat['ì‚¬ì—…ëª…'].tolist())
                selected_project = st.selectbox("3ë‹¨ê³„: ìƒì„¸ ì‚¬ì—…", proj_options)

                if selected_project == "ğŸ í•´ë‹¹ ì¤‘ë¶„ë¥˜ ì „ì²´ ì¢…í•©":
                    target_rows = projects_in_cat
                    display_title = f"[{selected_d2}] í•˜ìœ„ ì‚¬ì—… ì „ì²´ ë¶„ì„"
                else:
                    target_rows = df[df['ì‚¬ì—…ëª…'] == selected_project]
                    display_title = selected_project

    # ---------------------------------------------------------
    # [Main] ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½ ë° UI
    # ---------------------------------------------------------
    
    filter_metadata = {}
    if selected_d1 != "ğŸ” ì „ì²´ ë°ì´í„° (All RFPs)":
        filter_metadata['depth_1'] = selected_d1
    if selected_d2 and selected_d2 != "ğŸ“‚ í•´ë‹¹ ëŒ€ë¶„ë¥˜ ì „ì²´ ì¢…í•©":
        filter_metadata['depth_2'] = selected_d2
    if selected_project and selected_project != "ğŸ í•´ë‹¹ ì¤‘ë¶„ë¥˜ ì „ì²´ ì¢…í•©":
        filter_metadata['project_name'] = selected_project

    # [UI ë ˆì´ì•„ì›ƒ]
    col_info, col_chat = st.columns([1, 1.5])

    with col_info:
        st.subheader(f"ğŸ“Š {display_title}")
        st.caption(f"ì°¸ì¡° ë¬¸ì„œ: {len(target_rows)}ê±´")

    with col_chat:
        st.subheader("ğŸ’¬ AI ì»¨ì„¤í„´íŠ¸ ì§ˆì˜ì‘ë‹µ")
        
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # ì±„íŒ… ë¡œê·¸ ì¶œë ¥
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        # ì§ˆë¬¸ ì…ë ¥
        if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì´ ì¹´í…Œê³ ë¦¬ ì‚¬ì—…ë“¤ì˜ ê³µí†µì ì¸ ìê²© ìš”ê±´ì€ ë­ì•¼?)"):
            st.session_state.messages.append({"role": "user", "content": query})
            with st.chat_message("user"):
                st.markdown(query)

            # ë‹µë³€ ìƒì„±
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("â³ DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ì¤‘...")

                try:
                    # âœ… 1. Supabase ë²¡í„° ê²€ìƒ‰ (RAG í•µì‹¬)
                    # í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œ ì¤‘, ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ Top 5 ì²­í¬ë§Œ ê°€ì ¸ì˜´
                    retrieved_docs = db_manager.similarity_search(
                        query=query, 
                        filters=filter_metadata, # ì´ í•„í„°ëŠ” RPC í•¨ìˆ˜ êµ¬í˜„ì— ë”°ë¼ ì ìš© ë°©ì‹ì´ ë‹¤ë¦„
                        top_k=5
                    )
                    
                    if not retrieved_docs:
                        combined_context = "ê´€ë ¨ëœ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    else:
                        combined_context = db_manager.format_docs(retrieved_docs)
                        # ë””ë²„ê¹…: ê²€ìƒ‰ëœ ì²­í¬ ë³´ì—¬ì£¼ê¸° (ì„ íƒì‚¬í•­)
                        with st.expander("ğŸ” ê²€ìƒ‰ëœ RAG ì»¨í…ìŠ¤íŠ¸ í™•ì¸"):
                            st.write(combined_context)

                    # âœ… 2. í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
                    if builder:
                        final_messages = builder.build_messages(
                            category=selected_d1 if selected_d1 else "General",
                            title=display_title,
                            context=combined_context, # ì—¬ê¸°ê°€ ì´ì œ ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ì„
                            history=st.session_state.messages[:-1],
                            query=query
                        )
                    else:
                        # Fallback
                        final_messages = [
                            {"role": "system", "content": "ë‹¹ì‹ ì€ ì…ì°° ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                            {"role": "user", "content": f"ì°¸ê³ ë¬¸ì„œ:\n{combined_context}\n\nì§ˆë¬¸: {query}"}
                        ]

                    # âœ… 3. ë‹µë³€ ìƒì„± (ê¸°ì¡´ ë¡œì§ ë™ì¼)
                    response_text = model_manager.generate_response(
                        messages=final_messages,
                        source=source_key,
                        local_llm=local_llm,
                        openai_client=openai_client
                    )
                    
                    message_placeholder.markdown(response_text)
                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    message_placeholder.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()