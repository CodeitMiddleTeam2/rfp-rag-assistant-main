import os
import streamlit as st
from openai import OpenAI

#==============================================
# í”„ë¡œê·¸ë¨ëª…: model_manager.py
# í´ë”ìœ„ì¹˜: src/generation/model_manager.py
# í”„ë¡œê·¸ë¨ ì„¤ëª…: ì›¹ ë°ëª¨ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ê²Œë”(ë¡œì»¬ or API) ë§Œë“¤ì–´ì£¼ëŠ” ë§¤ë‹ˆì € í´ë˜ìŠ¤
# ì‘ì„±ì´ë ¥: 25.12.23 í•œìƒì¤€ ìµœì´ˆ ì‘ì„±
#===============================================

# ìºì‹±í•  í•¨ìˆ˜ëŠ” í´ë˜ìŠ¤ ë°–(ë˜ëŠ” staticmethod)ì— ì •ì˜í•©ë‹ˆë‹¤.
@st.cache_resource
def _load_llama_cpp_model(model_path: str, n_ctx: int = 24576):
    """
    ì‹¤ì œ ë¬´ê±°ìš´ ëª¨ë¸ ë¡œë”©ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ì˜ ë°˜í™˜ê°’(Llama ê°ì²´)ì´ ìºì‹±ë©ë‹ˆë‹¤.
    """
    try:
        from llama_cpp import Llama
        # print(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_path}") # ë””ë²„ê¹…ìš©
        llm = Llama(
            model_path=model_path,
            n_gpu_layers=-1, # L4 GPU í™œìš©
            n_ctx=n_ctx,
            verbose=True,
        )
        return llm
    except Exception as e:
        st.error(f"âŒ Llama ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

class ModelManager:
    def __init__(self, local_model_path: str = "../unsloth.Q4_K_M.gguf"):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.local_model_path = local_model_path

    def get_openai_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (ê°€ë²¼ìš´ ê°ì²´ë¼ ìºì‹± ë¶ˆí•„ìš”)"""
        if not self.api_key:
            st.error("ğŸš¨ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
        return OpenAI(api_key=self.api_key)

    def load_local_model(self):
        """
        í´ë˜ìŠ¤ ë©”ì„œë“œëŠ” ë‹¨ìˆœíˆ ìºì‹±ëœ ì „ì—­ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ì—­í• ë§Œ í•©ë‹ˆë‹¤.
        """
        if not os.path.exists(self.local_model_path):
            st.error(f"ğŸš¨ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.local_model_path}")
            return None
            
        # ì—¬ê¸°ì„œ ìºì‹±ëœ í•¨ìˆ˜ í˜¸ì¶œ (_load_llama_cpp_model)
        return _load_llama_cpp_model(self.local_model_path)

    def generate_response(self, messages, source="openai", local_llm=None, openai_client=None):
        """ë‹µë³€ ìƒì„± ë¡œì§ í†µí•©"""
        try:
            if source == "openai":
                if not openai_client:
                    return "ğŸš¨ OpenAI Clientê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
                response = openai_client.chat.completions.create(
                    model="gpt-5-nano",
                    messages=messages
                )
                return response.choices[0].message.content
            
            elif source == "local":
                if not local_llm:
                    return "ğŸš¨ ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
                # ë¡œì»¬ ëª¨ë¸ ì¶”ë¡ 
                response = local_llm.create_chat_completion(
                    messages=messages,
                    max_tokens=2048,
                    stop=["<|im_end|>", "<|endoftext|>", "User:"],
                    temperature=0.1
                )
                return response['choices'][0]['message']['content']
                
        except Exception as e:
            return f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"