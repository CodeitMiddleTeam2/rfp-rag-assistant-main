import os
import re
import torch
import gc
import streamlit as st
from openai import OpenAI
from langsmith import traceable

#==============================================
# í”„ë¡œê·¸ë¨ëª…: model_manager.py
# í´ë”ìœ„ì¹˜: src/generation/model_manager.py
# í”„ë¡œê·¸ë¨ ì„¤ëª…: ì›¹ ë°ëª¨ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ê²Œë”(ë¡œì»¬ or API) ë§Œë“¤ì–´ì£¼ëŠ” ë§¤ë‹ˆì € í´ë˜ìŠ¤
# ì‘ì„±ì´ë ¥: 25.12.23 í•œìƒì¤€ ìµœì´ˆ ì‘ì„±
# 25.12.29 ì •ê·œí‘œí˜„ì‹ ì „ì²˜ë¦¬ ì¶”ê°€
# 25.12.29 LangSmith ì¶”ì  ì¶”ê°€
#===============================================

# ìºì‹±í•  í•¨ìˆ˜ëŠ” í´ë˜ìŠ¤ ë°–(ë˜ëŠ” staticmethod)ì— ì •ì˜í•©ë‹ˆë‹¤.
@st.cache_resource
def _load_llama_cpp_model(model_path: str, n_ctx: int = 24576):
    """
    ì‹¤ì œ ë¬´ê±°ìš´ ëª¨ë¸ ë¡œë”©ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ì˜ ë°˜í™˜ê°’(Llama ê°ì²´)ì´ ìºì‹±ë©ë‹ˆë‹¤.
    """
    try:
        from llama_cpp import Llama
        # print(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_path}") # ë””ë²„ê¹…ìš©
        llm = Llama(
            model_path=model_path,
            n_gpu_layers=-1, # L4 GPU í™œìš©
            n_ctx=n_ctx,
            verbose=True,
        )
        return llm
    except Exception as e:
        st.error(f"âŒ Llama ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

class ModelManager:
    def __init__(self, local_model_path: str = "../unsloth.Q4_K_M.gguf"):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.local_model_path = local_model_path

    def get_openai_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (ê°€ë²¼ìš´ ê°ì²´ë¼ ìºì‹± ë¶ˆí•„ìš”)"""
        if not self.api_key:
            st.error("ğŸš¨ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
        return OpenAI(api_key=self.api_key)

    def load_local_model(self):
        """
        í´ë˜ìŠ¤ ë©”ì„œë“œëŠ” ë‹¨ìˆœíˆ ìºì‹±ëœ ì „ì—­ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ì—­í• ë§Œ í•©ë‹ˆë‹¤.
        """
        if not os.path.exists(self.local_model_path):
            st.error(f"ğŸš¨ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.local_model_path}")
            return None
            
        # ì—¬ê¸°ì„œ ìºì‹±ëœ í•¨ìˆ˜ í˜¸ì¶œ (_load_llama_cpp_model)
        return _load_llama_cpp_model(self.local_model_path)

    @traceable(run_type="llm", name="LLM_Generation")
    def generate_response(self, messages, source="openai", local_llm=None, openai_client=None):
        """ë‹µë³€ ìƒì„± ë¡œì§ í†µí•©"""
        try:
            if source == "openai":
                if not openai_client:
                    return "ğŸš¨ OpenAI Clientê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
                response = openai_client.chat.completions.create(
                    model="gpt-5-nano",
                    messages=messages
                )
                return response.choices[0].message.content
            
            elif source == "local":
                if not local_llm:
                    return "ğŸš¨ ë¡œì»¬ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
                # ë¡œì»¬ ëª¨ë¸ ì¶”ë¡ 
                response = local_llm.create_chat_completion(
                    messages=messages,
                    max_tokens=2048,
                    stop=["<|im_end|>", "<|endoftext|>", "User:"],
                    temperature=0.1
                )
                raw_content = response['choices'][0]['message']['content']

                # âœ… [í•µì‹¬ ìˆ˜ì •] <think> ... </think> íƒœê·¸ ì œê±° ë¡œì§
                # re.DOTALL: ì¤„ë°”ê¿ˆì´ í¬í•¨ëœ ë‚´ìš©ë„ ëª¨ë‘ ì°¾ìŒ
                clean_content = re.sub(r'<think>.*?</think>', '', raw_content, flags=re.DOTALL).strip()
                
                return clean_content
                
        except Exception as e:
            return f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"

    def clear_gpu_memory(self):
        """GPU ë©”ëª¨ë¦¬ ìºì‹œë¥¼ ê°•ì œë¡œ ë¹„ì›ë‹ˆë‹¤."""
        if torch.cuda.is_available():
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            # print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")