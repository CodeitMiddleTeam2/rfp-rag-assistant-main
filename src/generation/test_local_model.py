import os
from llama_cpp import Llama
import time

# 1. ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì •í™•í•œì§€ í™•ì¸!)
# ê°™ì€ í´ë”ì— ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ë‹¤ë¥´ë‹¤ë©´ ì ˆëŒ€ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.
MODEL_PATH = r"/home/spai0525/rfp-rag-assistant-main/unsloth.Q4_K_M.gguf"

def test_model():
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_PATH}")
    
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ì—ëŸ¬: íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ -> {MODEL_PATH}")
        return

    try:
        # 2. ëª¨ë¸ ì´ˆê¸°í™” (ë””ë²„ê¹…ì„ ìœ„í•´ verbose=True ì„¤ì •)
        # n_ctx=8192: í† í° ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ë„‰ë„‰í•˜ê²Œ ì¡ìŠµë‹ˆë‹¤.
        # n_gpu_layers=-1: ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì— ì˜¬ë¦½ë‹ˆë‹¤ (L4 GPU í•„ìˆ˜)
        llm = Llama(
            model_path=MODEL_PATH,
            n_gpu_layers=-1, 
            n_ctx=8192,      
            verbose=True      # í„°ë¯¸ë„ì— GPU ë¡œë“œ ë¡œê·¸ê°€ ì°í™ë‹ˆë‹¤.
        )
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

        # 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„±
        # Qwen 2.5 Instruct ëª¨ë¸ì€ ChatML í¬ë§·ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": "B2G ì…ì°° ì œì•ˆì„œ(RFP)ë¥¼ ì‘ì„±í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ëŠ” ë­ì•¼?"}
        ]

        print("\nğŸ’¬ ì§ˆë¬¸ ì…ë ¥ ì¤‘...")
        print(f"Q: {messages[1]['content']}\n")

        # 4. ì¶”ë¡  ì‹œì‘ (ì‹œê°„ ì¸¡ì •)
        start_time = time.time()
        
        output = llm.create_chat_completion(
            messages=messages,
            max_tokens=512,       # ë‹µë³€ ê¸¸ì´ ì œí•œ
            temperature=0.3,      # ì°½ì˜ì„± ì¡°ì ˆ
            stop=["<|im_end|>", "<|endoftext|>"], # [ì¤‘ìš”] ì´ê²Œ ì—†ìœ¼ë©´ ë¬´í•œ ë¡œë”© ê±¸ë¦¼
            stream=True           # í•œ ê¸€ìì”© ì¶œë ¥ í…ŒìŠ¤íŠ¸
        )

        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘:", end=" ", flush=True)
        
        full_response = ""
        for chunk in output:
            delta = chunk['choices'][0]['delta']
            if 'content' in delta:
                token = delta['content']
                print(token, end="", flush=True)
                full_response += token
        
        end_time = time.time()
        print(f"\n\nâ±ï¸ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print("âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        # í† í° ì—ëŸ¬ë¼ë©´ ë³´í†µ ì—¬ê¸°ì„œ ValueErrorê°€ ëœ¹ë‹ˆë‹¤.

if __name__ == "__main__":
    test_model()